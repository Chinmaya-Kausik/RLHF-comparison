{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f38929-cede-4dee-8ade-5b33f45ff738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset loaded\n",
      "model and tokenizer initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ckausik/.local/lib/python3.9/site-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "/home/ckausik/.local/lib/python3.9/site-packages/trl/trainer/kto_trainer.py:251: UserWarning: When using DPODataCollatorWithPadding, you should set `max_length` in the KTOTrainer's init it will be set to `512` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/ckausik/.local/lib/python3.9/site-packages/trl/trainer/kto_trainer.py:261: UserWarning: When using DPODataCollatorWithPadding, you should set `max_prompt_length` in the KTOTrainer's init it will be set to `128` by default, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/ckausik/.local/lib/python3.9/site-packages/trl/trainer/kto_trainer.py:291: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your KTOConfig we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/home/ckausik/.local/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer initialised\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from numpy import random\n",
    "from trl import KTOConfig, KTOTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "kto_train_dataset = load_from_disk(\"/home/ckausik/kto_train_dataset\")\n",
    "print(\"dataset loaded\")\n",
    "\n",
    "tldr_sft = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\", device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"model and tokenizer initialized\")\n",
    "\n",
    "# tldr_sft = AutoModelForCausalLM.from_pretrained(\"CarperAI/openai_summarize_tldr_sft\", device_map=\"auto\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "training_args = KTOConfig(output_dir = \"/home/ckausik\",\n",
    "    beta=0.1,\n",
    "    desirable_weight=1.0,\n",
    "    undesirable_weight=1.0, report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "kto_trainer = KTOTrainer(\n",
    "    tldr_sft,\n",
    "    args=training_args,\n",
    "    train_dataset=kto_train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config = lora_config\n",
    ")\n",
    "\n",
    "print(\"trainer initialised\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99967cf8-41b1-4e79-916f-774f54761e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kto_train_dataset[0][\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2191d0c7-0f39-410f-b90b-e18bf13e746b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/ckausik/.local/lib/python3.9/site-packages (0.16.5)\n",
      "Requirement already satisfied: PyYAML in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from wandb) (5.8.0)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/ckausik/.local/lib/python3.9/site-packages (from wandb) (3.1.42)\n",
      "Requirement already satisfied: setproctitle in /home/ckausik/.local/lib/python3.9/site-packages (from wandb) (1.3.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/ckausik/.local/lib/python3.9/site-packages (from wandb) (1.44.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /home/ckausik/.local/lib/python3.9/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: setuptools in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from wandb) (58.0.4)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from wandb) (2.26.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ckausik/.local/lib/python3.9/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: typing-extensions in /home/ckausik/.local/lib/python3.9/site-packages (from wandb) (4.8.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ckausik/.local/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ckausik/.local/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ckausik/.local/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /sw/pkgs/arc/python3.9-anaconda/2021.11/lib/python3.9/site-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a599b50-15c4-429a-9c91-179f0f3ad105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'GPT2LMHeadModel' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=\"gpt2-medium\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1d43702-8714-4d4f-ba8a-233034ade981",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "text = kto_train_dataset[0][\"prompt\"]\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "inputs[\"attention_mask\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5919198-8cd7-44c9-b0f7-443720934abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# inputs.input_ids.to(device)\n",
    "outputs = tldr_sft.generate(inputs['input_ids'].to(device), max_new_tokens=100, do_sample=False, pad_token_id = tokenizer.eos_token_id, attention_mask = inputs['attention_mask'].to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb936476-69b7-40ec-bd12-bc29fd47a861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\n",
      "\n",
      "The plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \n",
      "\n",
      "I am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\n",
      "\n",
      "\n",
      " hehehehehe\n",
      "[\"My boyfriend and I are long distance. We have a trip planned this summer which involves me going over to him in the USA. This will be the second time I have actually been with him in person. I am flying from the UK with my mum to the east coast. The original plan was for me to fly over to my boyfriend in the west coast (my parents are holidaying on the east coast) but because my mum was freaking out so much about me going to meet my boyfriend i said we can all road trip there together. I even invited her on the trip with us. I have given her all of our dates so that she can travel around with us.\\n\\nThe plan was for me to stay on the 4th July and fly back on the 5th. Mum knew this. I told her I had booked a flight back already from the west coast to east coast (where she would pick me up and we would fly back to the UK together). She has gone mad at me because she can't believe I would book a flight when she told me she didn't want me flying on my own. At the time I had booked it she told me she wasn't gonna road trip with us. She knew the trip was happening.......how else was I to get home if I don't fly? \\n\\nI am fine flying on my own it doesn't bother me at all. I feel like I have done everything I can to make her feel comfortable with this trip and she is just trying to sabotage it. Thoughts??\\n\\nI am not sure if this is a good idea or not. I have been flying with my boyfriend for a few months now and I have been flying with him for a few months now. I have been flying with him for a few months now. I have been flying with him for a few months now. I have been flying with him for a few months now. I have been flying with him for a few months now. I have been flying with him for a few months now.\"]\n"
     ]
    }
   ],
   "source": [
    "print(text)\n",
    "print(\"\\n\\n hehehehehe\")\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d6f5c30-3234-40ae-b5cc-9c00765daa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from numpy import random\n",
    "from trl import KTOConfig, KTOTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2869873a-0f55-470f-aaf4-7da3b3899d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 1024)\n",
      "    (wpe): Embedding(1024, 1024)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-23): 24 x GPT2Block(\n",
      "        (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "print(model.base_model)\n",
    "# for layer in model.base_model:\n",
    "#     print(i)\n",
    "#     i +=i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1eaa0f-6c03-450d-975d-f982308f5f30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
